# Neural Network Implementation & Comparative Study

## ğŸ“Œ Overview
This project implements a **scratch-built Multi-Layer Perceptron (MLP)** using NumPy and compares its performance against other models, including **Keras Neural Networks, XGBoost, Linear Regression, and Random Forest**.  

The study explores how different **architectures, activation functions, and optimizers** influence performance and robustness under varying data conditions.

---

## ğŸ¯ Goals
- Implement a **neural network from scratch** with forward and backward propagation.
- Compare optimizers: **SGD, Momentum, Adam**.
- Study the effect of **depth and activation functions** (ReLU, Tanh, LeakyReLU).
- Analyze robustness with:
  - **Noisy training data**
  - **Reduced training samples**
- Benchmark against:
  - **Linear Regression**
  - **Random Forest**
  - **XGBoost**
  - **Keras Neural Network**

---

## âš™ï¸ Implemented Components

### ğŸ”¹ Optimizers
- **SGD** â€“ simple gradient descent update.  
- **Momentum** â€“ accelerated updates using past gradients.  
- **Adam** â€“ adaptive learning rate + momentum.

### ğŸ”¹ Activation Functions
- **ReLU** â€“ efficient, avoids vanishing gradients.  
- **Tanh** â€“ zero-centered, outputs in [-1, 1].  
- **Leaky ReLU** â€“ variant of ReLU with small negative slope to prevent dead neurons.

### ğŸ”¹ Loss Function
- **MSE with L2 regularization** â€“ balances error minimization with generalization.

### ğŸ”¹ Evaluation Metrics
- **MSE** â€“ Mean Squared Error  
- **RMSE** â€“ Root Mean Squared Error  
- **MAE** â€“ Mean Absolute Error  
- **RÂ²** â€“ Coefficient of Determination

---

## ğŸ“Š Workflow
1. **Forward Propagation** â€“ compute layer outputs.  
2. **Loss Calculation** â€“ quantify prediction error.  
3. **Backpropagation** â€“ compute gradients.  
4. **Optimizer Step** â€“ update weights.  
5. **Training Loop** â€“ repeat across epochs.  
6. **Evaluation** â€“ compare performance across models.

---

## ğŸ” Key Insights
- Custom MLP achieves competitive performance but depends on optimizer and activation choice.  
- **Adam** generally provides the most stable and fastest convergence.  
- **ReLU and Leaky ReLU** perform better than Tanh in deeper architectures.  
- Traditional ML models like **Random Forest** and **XGBoost** can rival neural networks on structured/tabular data.

---

## ğŸ“‚ Repository Structure
.
â”œâ”€â”€ NN_uupdates.ipynb # Main Jupyter notebook (implementation & experiments)
â”œâ”€â”€ data/ # Dataset(s) used
â”œâ”€â”€ results/ # Metrics, plots, and comparison outputs
â””â”€â”€ README.md # Project overview


---

## ğŸ› ï¸ Dependencies
- Python 3.x  
- NumPy  
- Matplotlib  
- Scikit-learn  
- XGBoost  
- TensorFlow / Keras  

Install required packages:
```bash
pip install numpy matplotlib scikit-learn xgboost tensorflow


## Run instruction
- Clone the repository and navigate to the project directory.
- Open the Jupyter notebook:
    jupyter notebook NN_uupdates.ipynb
- Run all cells to train models and view results.
